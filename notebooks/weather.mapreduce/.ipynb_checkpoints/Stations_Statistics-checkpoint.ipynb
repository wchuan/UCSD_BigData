{
 "metadata": {
  "name": "",
  "signature": "sha256:64a0aa8931e3c9f98182034cdf1b7d5d6bb9d3ad10c56bb3340e4dea26603a44"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sklearn as sk\n",
      "print 'pandas version: ',pd.__version__\n",
      "print 'numpy version:',np.__version__\n",
      "print 'sklearn version:',sk.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "pandas version:  0.13.1\n",
        "numpy version: 1.8.1\n",
        "sklearn version: 0.14.1\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys,os\n",
      "cwd=os.getcwd()\n",
      "path=cwd.split('/')\n",
      "home_dir='/'.join(path[:-2])\n",
      "print home_dir\n",
      "sys.path.append(home_dir+'/utils')\n",
      "from find_waiting_flow import *\n",
      "from AWS_keypair_management import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/ubuntu/UCSD_BigData\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = AWS_keypair_management()\n",
      "(Creds,bad_files) = A.Get_Working_Credentials('/home/ubuntu/Vault')\n",
      "pair = Creds['chw124']['Creds'][0]\n",
      "key_id = pair['Access_Key_Id']\n",
      "secret_key = pair['Secret_Access_Key']\n",
      "job_flow_id = find_waiting_flow(key_id,secret_key)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "credentials.csv AWS creds: chw124 AKIAIUULOQ2W5CK2S4CQ\n",
        "an active key pair"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<boto.emr.emrobject.JobFlow object at 0x406b3d0>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " no_script.yoavfreund.20140516.040032.370095 j-262J0JTFJIRLO WAITING\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile ECatch.py\n",
      "import traceback\n",
      "from functools import wraps\n",
      "from sys import stderr\n",
      "\"\"\"this decorator is intended for decorating a function, not a\n",
      "generator.  Therefor to use it in the context of mrjob, the generator\n",
      "should call a function that handles a single input records, and that\n",
      "function should be decorated.\n",
      "\n",
      "The reason is that if a generator throws an exception it exits and\n",
      "cannot process any more records.\n",
      "\n",
      "\"\"\"\n",
      "def ECatch(func):\n",
      "    print type(func)\n",
      "    f_name=func.__name__\n",
      "    @wraps(func)\n",
      "    def inner(self,*args,**kwargs):\n",
      "        try:\n",
      "            self.increment_counter(self.__class__.__name__,'total in '+f_name,1)\n",
      "            return func(self,*args,**kwargs)\n",
      "        except Exception as e:\n",
      "            self.increment_counter(self.__class__.__name__,'errors in '+f_name,1)\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e))\n",
      "            traceback.print_exc(file=stderr)\n",
      "            stderr.write('Arguments were %s, %s\\n'%(args,kwargs))\n",
      "            pass\n",
      "    return inner        \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting ECatch.py\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from ECatch import *\n",
      "class C:\n",
      "    def increment_counter(self,a,b,n):\n",
      "        print 'increment counter(',a,b,n,')'\n",
      "    @ECatch\n",
      "    def Z(self,list):\n",
      "        print list\n",
      "        print sum(list)\n",
      "        return sum(list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'function'>\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile Stations_Statistics.py\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "collect the statistics for each station.\n",
      "\"\"\"\n",
      "import re,pickle,base64,zlib\n",
      "from sys import stderr\n",
      "import sys\n",
      "\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages') # a hack because anaconda made mrjob unreachable\n",
      "from mrjob.job import MRJob\n",
      "from mrjob.protocol import *\n",
      "\n",
      "import traceback\n",
      "from functools import wraps\n",
      "from sys import stderr\n",
      "\n",
      "\"\"\"this decorator is intended for decorating a function, not a\n",
      "generator.  Therefor to use it in the context of mrjob, the generator\n",
      "should call a function that handles a single input records, and that\n",
      "function should be decorated.\n",
      "\n",
      "The reason is that if a generator throws an exception it exits and\n",
      "cannot process any more records.\n",
      "\n",
      "\"\"\"\n",
      "def ECatch(func):\n",
      "    f_name=func.__name__\n",
      "    @wraps(func)\n",
      "    def inner(self,*args,**kwargs):\n",
      "        try:\n",
      "            self.increment_counter(self.__class__.__name__,'total in '+f_name,1)\n",
      "            return func(self,*args,**kwargs)\n",
      "        except Exception as e:\n",
      "            self.increment_counter(self.__class__.__name__,'errors in '+f_name,1)\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e))\n",
      "            traceback.print_exc(file=stderr)\n",
      "            stderr.write('Arguments were %s, %s\\n'%(args,kwargs))\n",
      "            pass\n",
      "    return inner        \n",
      "\n",
      "\"\"\"\n",
      "Functions for encoding and decoding arbitrary object into ascii \n",
      "so that they can be passed through the hadoop streaming interface.\n",
      "\"\"\"\n",
      "\n",
      "def loads(eVal):\n",
      "    \"\"\" Decode a string into a value \"\"\"\n",
      "    return pickle.loads(zlib.decompress(base64.b64decode(eVal)))\n",
      "\n",
      "def dumps(Value):\n",
      "    \"\"\" Encode a value as a string \"\"\"\n",
      "    return base64.b64encode(zlib.compress(pickle.dumps(Value),9))\n",
      "\n",
      "class MRWeather(MRJob):\n",
      "\n",
      "    @ECatch\n",
      "    def map_one(self,line):\n",
      "        return line.split(',')\n",
      "    \n",
      "    def mapper(self, _, line):\n",
      "        elements=self.map_one(line)\n",
      "        yield(elements[0],elements[1:])\n",
      "            \n",
      "    def check_integrity(self,meas,year,length):\n",
      "        year=int(year)\n",
      "        if year<1000 or year > 2014: return False\n",
      "        if meas=='': return False\n",
      "        if length != 367: return False\n",
      "        return True\n",
      "    \n",
      "    @ECatch\n",
      "    def reduce_one(self,S,vector):\n",
      "        meas=vector[0]\n",
      "        year=vector[1]\n",
      "        length=len(vector)\n",
      "        number_defined=sum([e!='' for e in vector[2:]])\n",
      "        assert self.check_integrity(meas,year,length)==True\n",
      "        S[(meas,int(year))]=number_defined\n",
      "        \n",
      "    def reducer(self, station, vectors):\n",
      "        S={}\n",
      "        for vector in vectors:\n",
      "            self.reduce_one(S,vector)\n",
      "        yield(station,dumps(S))\n",
      "                              \n",
      "if __name__ == '__main__':\n",
      "    MRWeather.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting Stations_Statistics.py\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Running job inline"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -1 $data_dir/ALL.head.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ASN00054128,DAPR,1969,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\r\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python Stations_Statistics.py --help"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Usage: Stations_Statistics.py [options] [input files]\r\n",
        "\r\n",
        "Options:\r\n",
        "  --help                show this message and exit\r\n",
        "  --help-emr            show EMR-related options\r\n",
        "  --help-hadoop         show Hadoop-related options\r\n",
        "  --help-runner         show runner-related options\r\n",
        "\r\n",
        "  Running specific parts of the job:\r\n",
        "    --mapper            run a mapper\r\n",
        "    --combiner          run a combiner\r\n",
        "    --reducer           run a reducer\r\n",
        "    --step-num=STEP_NUM\r\n",
        "                        which step to execute (default is 0)\r\n",
        "    --steps             print the mappers, combiners, and reducers that this\r\n",
        "                        job defines\r\n",
        "\r\n",
        "  Protocols:\r\n",
        "    --strict-protocols  If something violates an input/output protocol then\r\n",
        "                        raise an exception\r\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!python Stations_Statistics.py $data_dir/ALL.corrupted.csv > StationStatistics.pkl\n",
      "!python Stations_Statistics.py $data_dir/ALL.head.csv > StationStatistics.pkl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n",
        "creating tmp directory /tmp/Stations_Statistics.ubuntu.20140519.222304.883241\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "writing to /tmp/Stations_Statistics.ubuntu.20140519.222304.883241/step-0-mapper_part-00000\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters from step 1:\r\n",
        "  MRWeather:\r\n",
        "    total in map_one: 999\r\n",
        "writing to /tmp/Stations_Statistics.ubuntu.20140519.222304.883241/step-0-mapper-sorted\r\n",
        "> sort /tmp/Stations_Statistics.ubuntu.20140519.222304.883241/step-0-mapper_part-00000\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "writing to /tmp/Stations_Statistics.ubuntu.20140519.222304.883241/step-0-reducer_part-00000\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters from step 1:\r\n",
        "  MRWeather:\r\n",
        "    total in map_one: 999\r\n",
        "    total in reduce_one: 999\r\n",
        "Moving /tmp/Stations_Statistics.ubuntu.20140519.222304.883241/step-0-reducer_part-00000 -> /tmp/Stations_Statistics.ubuntu.20140519.222304.883241/output/part-00000\r\n",
        "Streaming final output from /tmp/Stations_Statistics.ubuntu.20140519.222304.883241/output\r\n",
        "removing tmp directory /tmp/Stations_Statistics.ubuntu.20140519.222304.883241\r\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!python Stations_Statistics.py -r emr --emr-job-flow-id $job_flow_id $data_dir/ALL.corrupted.csv > StationStatistics.pkl\n",
      "!python Stations_Statistics.py -r emr --emr-job-flow-id $job_flow_id $data_dir/ALL.head.csv > StationStatistics.pkl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating tmp directory /tmp/Stations_Statistics.ubuntu.20140519.222309.631822\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Copying non-input files into s3://yoav.hadoop/scratch/Stations_Statistics.ubuntu.20140519.222309.631822/files/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adding our job to existing job flow j-262J0JTFJIRLO\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 30.2s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.222309.631822: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 60.5s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.222309.631822: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job completed.\r\n",
        "Running time was 67.0s (not counting time spent waiting for the EC2 instances)\r\n",
        "ec2_key_pair_file not specified, going to S3\r\n",
        "Fetching counters from S3...\r\n",
        "Waiting 5.0s for S3 eventual consistency\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters from step 1:\r\n",
        "  File Input Format Counters :\r\n",
        "    Bytes Read: 1052615\r\n",
        "  File Output Format Counters :\r\n",
        "    Bytes Written: 11195\r\n",
        "  FileSystemCounters:\r\n",
        "    FILE_BYTES_READ: 401879\r\n",
        "    FILE_BYTES_WRITTEN: 2686946\r\n",
        "    HDFS_BYTES_READ: 8760\r\n",
        "    S3_BYTES_READ: 1052615\r\n",
        "    S3_BYTES_WRITTEN: 11195\r\n",
        "  Job Counters :\r\n",
        "    Launched map tasks: 60\r\n",
        "    Launched reduce tasks: 8\r\n",
        "    Rack-local map tasks: 60\r\n",
        "    SLOTS_MILLIS_MAPS: 462272\r\n",
        "    SLOTS_MILLIS_REDUCES: 117796\r\n",
        "    Total time spent by all maps waiting after reserving slots (ms): 0\r\n",
        "    Total time spent by all reduces waiting after reserving slots (ms): 0\r\n",
        "  MRWeather:\r\n",
        "    total in map_one: 999\r\n",
        "    total in reduce_one: 999\r\n",
        "  Map-Reduce Framework:\r\n",
        "    CPU time spent (ms): 55930\r\n",
        "    Combine input records: 0\r\n",
        "    Combine output records: 0\r\n",
        "    Map input bytes: 858960\r\n",
        "    Map input records: 999\r\n",
        "    Map output bytes: 1963854\r\n",
        "    Map output materialized bytes: 408690\r\n",
        "    Map output records: 999\r\n",
        "    Physical memory (bytes) snapshot: 26670911488\r\n",
        "    Reduce input groups: 11\r\n",
        "    Reduce input records: 999\r\n",
        "    Reduce output records: 11\r\n",
        "    Reduce shuffle bytes: 408690\r\n",
        "    SPLIT_RAW_BYTES: 8760\r\n",
        "    Spilled Records: 1998\r\n",
        "    Total committed heap usage (bytes): 26886012928\r\n",
        "    Virtual memory (bytes) snapshot: 127522160640\r\n",
        "Streaming final output from s3://yoav.hadoop/scratch/Stations_Statistics.ubuntu.20140519.222309.631822/output/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "removing tmp directory /tmp/Stations_Statistics.ubuntu.20140519.222309.631822\r\n",
        "Removing all files in s3://yoav.hadoop/scratch/Stations_Statistics.ubuntu.20140519.222309.631822/\r\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python Stations_Statistics.py -r emr --emr-job-flow-id  $job_flow_id  hdfs:/weather/weather.csv > Statistics.pkl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating tmp directory /tmp/Stations_Statistics.ubuntu.20140519.231243.441500\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Copying non-input files into s3://yoav.hadoop/scratch/Stations_Statistics.ubuntu.20140519.231243.441500/files/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adding our job to existing job flow j-262J0JTFJIRLO\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 30.2s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 60.5s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 90.7s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 121.0s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 151.3s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 181.6s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 211.9s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 242.2s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 272.4s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 302.6s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 332.9s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 363.1s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 393.4s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 423.6s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 453.9s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 484.1s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 514.4s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 544.6s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 574.9s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 605.1s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 635.4s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 665.6s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 695.9s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 726.2s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 756.4s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 786.7s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140519.231243.441500: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job completed.\r\n",
        "Running time was 777.0s (not counting time spent waiting for the EC2 instances)\r\n",
        "ec2_key_pair_file not specified, going to S3\r\n",
        "Fetching counters from S3...\r\n",
        "Waiting 5.0s for S3 eventual consistency\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters from step 1:\r\n",
        "  File Input Format Counters :\r\n",
        "    Bytes Read: 7670788923\r\n",
        "  File Output Format Counters :\r\n",
        "    Bytes Written: 107043468\r\n",
        "  FileSystemCounters:\r\n",
        "    FILE_BYTES_READ: 6942513297\r\n",
        "    FILE_BYTES_WRITTEN: 10374506158\r\n",
        "    HDFS_BYTES_READ: 7670794863\r\n",
        "    S3_BYTES_WRITTEN: 107043468\r\n",
        "  Job Counters :\r\n",
        "    Data-local map tasks: 61\r\n",
        "    Launched map tasks: 67\r\n",
        "    Launched reduce tasks: 10\r\n",
        "    Rack-local map tasks: 6\r\n",
        "    SLOTS_MILLIS_MAPS: 4027219\r\n",
        "    SLOTS_MILLIS_REDUCES: 3296592\r\n",
        "    Total time spent by all maps waiting after reserving slots (ms): 0\r\n",
        "    Total time spent by all reduces waiting after reserving slots (ms): 0\r\n",
        "  MRWeather:\r\n",
        "    errors in reduce_one: 1\r\n",
        "    total in map_one: 9358395\r\n",
        "    total in reduce_one: 9358395\r\n",
        "  Map-Reduce Framework:\r\n",
        "    CPU time spent (ms): 4704230\r\n",
        "    Combine input records: 0\r\n",
        "    Combine output records: 0\r\n",
        "    Map input bytes: 7668890105\r\n",
        "    Map input records: 9358395\r\n",
        "    Map output bytes: 18019274975\r\n",
        "    Map output materialized bytes: 3457612075\r\n",
        "    Map output records: 9358395\r\n",
        "    Physical memory (bytes) snapshot: 37379256320\r\n",
        "    Reduce input groups: 85284\r\n",
        "    Reduce input records: 9358395\r\n",
        "    Reduce output records: 85284\r\n",
        "    Reduce shuffle bytes: 3457612075\r\n",
        "    SPLIT_RAW_BYTES: 5940\r\n",
        "    Spilled Records: 28075185\r\n",
        "    Total committed heap usage (bytes): 35039215616\r\n",
        "    Virtual memory (bytes) snapshot: 127122763776\r\n",
        "Streaming final output from s3://yoav.hadoop/scratch/Stations_Statistics.ubuntu.20140519.231243.441500/output/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "removing tmp directory /tmp/Stations_Statistics.ubuntu.20140519.231243.441500\r\n",
        "Removing all files in s3://yoav.hadoop/scratch/Stations_Statistics.ubuntu.20140519.231243.441500/\r\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coding.py\t\t\t README.txt\r\n",
        "counts\t\t\t\t reduce-year-temp.py\r\n",
        "Description of Assignment.ipynb  stations.pkl\r\n",
        "ECatch.py\t\t\t stations.pkl.gz\r\n",
        "ECatch.pyc\t\t\t Stations_Statistics.ipynb\r\n",
        "Eigen-by-Station.sh\t\t Stations_Statistics.py\r\n",
        "kdTree.py\t\t\t StationStatistics.pkl\r\n",
        "map-year-temp.py\t\t Statistics.pkl\r\n",
        "mrjob and EMR.ipynb\t\t Statistics.py\r\n",
        "mr_weather.py\t\t\t weather_MRjob.ipynb\r\n",
        "mr_word_freq_count.py\t\t weather_MRjob-with-utils.ipynb\r\n",
        "PCA_mr.py\r\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "s = open('Statistics.pkl')\n",
      "f = re.split('\"[\\W\"]*',line)\n",
      "i = 0\n",
      "for line in s.readlines():\n",
      "    i += 1\n",
      "    if i <= \n",
      "    f = re.split('\"[\\W\"]*',line)\n",
      "    if len(f) != 4: print f"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = re.split('\"[\\W\"]*',line)\n",
      "print f\n",
      "print len(f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['', 'AJ000037668', 'eNpdkjGOQyEMRHtOkmoF2AZTb5Uu2mJPkAN8Kbm/lhlrFeT6Cfsx49vzquX2+/j5fpSrlXtbruV99XKXYeX1TwRk+ia6iRzESGSTgTf9QybImJs4SP2QBbLaJq2mRY0OY4H1NLDRwjGxaX5HD+fMkRlNZgeDiunB6DIG/gyXdrBOl4l9PQfS6bIqWHbpdBlIq9OlHYwuzn2e39HFMVNyLhIuBkYXOVi0g5lCl+MPEv0gF8m5SDTEmczFDxYu6EGzi8adoHOFixz7NFxwQwoXO99FR9in2UWjI+7z9Aely0QuVtNNWNwL9llP3VrcC3qw3JGFC27C4LJDfX39AeOPtXw=', '']\n",
        "4\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re,pickle,base64,zlib\n",
      "def loads(eVal):\n",
      "    \"\"\" Decode a string into a value \"\"\"\n",
      "    return pickle.loads(zlib.decompress(base64.b64decode(eVal)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = loads(f[2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{(u'PRCP', 1984): 365, (u'PRCP', 1978): 335, (u'PRCP', 1973): 362, (u'PRCP', 1967): 360, (u'PRCP', 1991): 365, (u'PRCP', 1969): 362, (u'PRCP', 1987): 365, (u'PRCP', 1981): 365, (u'PRCP', 1972): 354, (u'PRCP', 1966): 314, (u'PRCP', 1977): 365, (u'PRCP', 1990): 365, (u'PRCP', 1968): 361, (u'PRCP', 1986): 365, (u'PRCP', 1980): 365, (u'PRCP', 1975): 363, (u'PRCP', 1976): 364, (u'PRCP', 1971): 365, (u'PRCP', 1965): 358, (u'PRCP', 1989): 365, (u'PRCP', 1983): 334, (u'PRCP', 1974): 355, (u'PRCP', 1985): 365, (u'PRCP', 1979): 364, (u'PRCP', 1970): 362, (u'PRCP', 1964): 354, (u'PRCP', 1988): 365, (u'PRCP', 1982): 332}\n"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Valid Station\n",
      "\n",
      "# mapper:\n",
      "    data => [station, year], [data]\n",
      "# reducer1:\n",
      "[station, year], vector[data] => station, 1/0\n",
      "1) if len(vector[data]) >= 2: OK\n",
      "2) if data1 and data2 data1[i]!= '' && data2[i] != '': OK    \n",
      "\n",
      "# reducer2:\n",
      "station, 1/0 => station\n",
      "1) len(1)/len(1+0) > 50% : OK\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Data Partition\n",
      "\n",
      "Node:\n",
      "    0 0 is latitude 1 is longitude\n",
      "    1 latitude\n",
      "    2 longitude\n",
      "    3 isleaf\n",
      "    4 depth\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PCA\n",
      "\n",
      "mapper:\n",
      "    station => key\n",
      "    x = x - mean\n",
      "    data => [key, x*x^T]\n",
      "reducer:\n",
      "    1/n sum x*x^T\n",
      "    PCA cov matrix\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class StationValid(MRJob):\n",
      "    def "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}